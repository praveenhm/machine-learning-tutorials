{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T05:41:05.805964Z",
     "start_time": "2021-10-15T05:41:05.786820Z"
    },
    "cell_style": "center",
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run common-imports.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "#  Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "an effective way to optimize the loss function, and thus make the algorithm learn an effective model for estimations on a dataset $\\mathscr{D}$ comprised of predictors and response. will do the process of learning an effective model through  gradient descent, by working out the steps. For simplicity, we will consider the case of ordinary least squares regression (simple linear regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:08:57.148666Z",
     "start_time": "2021-10-14T20:08:56.793731Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"../datasets/old-faithful-geyser.csv\", sep='\\t')\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(raw_data)\n",
    "data = pd.DataFrame(data={'eruptions':scaled[:, 0], 'waiting': scaled[:,1]})\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first visualize a scatter plot of the data and look at the histogram of the features. Finally, let us pull of this together with a kernel density plot, into a single plot with subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:08:58.499168Z",
     "start_time": "2021-10-14T20:08:57.730221Z"
    },
    "cell_style": "split",
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data['eruptions'], data['waiting'], alpha=0.5, s=150, color='salmon')\n",
    "plt.title(r'\\textbf{Scatter-plot of $waiting$ vs $eruptions$}')\n",
    "plt.xlabel(r'eruptions $\\longrightarrow$');\n",
    "plt.ylabel(r'waiting $\\longrightarrow$');\n",
    "plt.show();\n",
    "data.hist(bins=50, alpha = 0.4, color='salmon', xrot=90, figsize=(10,5));\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:08:59.933401Z",
     "start_time": "2021-10-14T20:08:58.545080Z"
    },
    "cell_style": "split",
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_palette(\"Reds\")\n",
    "\n",
    "sample = data\n",
    "g = sns.pairplot(sample, diag_kind='kde', \n",
    "                   plot_kws = { 'alpha': 0.20, 's': 80, 'edgecolor': 'k', 'color':'salmon'}, \n",
    "                   size=5, );\n",
    "g.map_diag(sns.kdeplot, color='salmon',  shade=True);\n",
    "g.map_upper(plt.scatter, color='salmon', alpha=0.5);\n",
    "g.map_lower(sns.kdeplot, shade=False, shade_lowest=False, cbar=True);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent from first principles\n",
    "\n",
    "Recall that a step of learning is given by the equation:\n",
    "$$\\begin{align}\\boldsymbol{\\beta_\\text{next}} &= \\boldsymbol{\\beta} - \\alpha \\nabla \\mathscr {L(\\mathbf\\beta)} \\\\\n",
    "\\text{where} &: \\nonumber\\\\\n",
    "\\boldsymbol{\\beta} &: \\text{the parameter vector} \\begin{pmatrix} \\beta_0\\\\ \\beta_1 \\end{pmatrix}\\nonumber\\\\\n",
    "\\boldsymbol{\\beta_\\text{next}} &: \\text{the next value of the parameter vector} \\nonumber\\\\\n",
    "\\alpha &: \\text{the learning rate} \\nonumber\\\\\n",
    "\\mathscr{L(\\mathbf\\beta)} &: \\text{the loss function}\\nonumber\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Let us consider an ordinary least squares loss function, so that we define the loss as simply the sum-squared errors.\n",
    "\n",
    "$$\\begin{align}\\mathscr{L} &= \\sum_{i=1}^n (y_i - \\widehat{y}_i)^2\\\\\n",
    "&= \\sum_{i=1}^n (y_i - \\beta_0 -\\beta_1 x_i)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The expression for the gradient of the loss is, therefore:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla \\mathscr {L}(\\mathbf\\beta) = \\begin{pmatrix} \\frac{\\partial L}{\\partial\\beta_0} \\\\ \\frac{\\partial L}{\\partial\\beta_1} \\end{pmatrix} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let us work out each of the component derivatives:\n",
    "\n",
    "$$\\begin{align}\\frac{\\partial \\mathscr L}{\\partial \\beta_0} &= - \\sum_{i=1}^n (y_i -\\beta_0 - \\beta_1 x_i)\\\\\n",
    "\\frac{\\partial \\mathscr L}{\\partial \\beta_1} &= - \\sum_{i=1}^n x_i(y_i -\\beta_0 - \\beta_1 x_i)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Therefore, the gradient descent step can be expressed as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\beta_{0, next} &= \\beta_0 + \\alpha\\sum_{i=1}^n (y_i -\\beta_0 - \\beta_1 x_i) \\\\\n",
    "\\beta_{1, next} &= \\beta_1 + \\alpha\\sum_{i=1}^n x_i(y_i -\\beta_0 - \\beta_1 x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We are going to perform a batch gradient descent, i.e. use all of the training data to compute loss in each step. Let us start with a small learning rate, say $\\alpha = 10^{-5}$. We need a stopping criterion for our learning process.\n",
    "\n",
    "For simplicity, let us stop after say 200 epochs, i.e. running the gradient descent for a two hundred steps.\n",
    "\n",
    "**Definition: EPOCH**\n",
    "While training, an epoch is a complete cycle through the entire dataset. In other words, each datum in the dataset should have contributed to the learning in that cycle.\n",
    "\n",
    "For simple or batch gradient descent, we use all of the data in each cycle; therefore, one step of gradient descent maps to an entire epoch. Later we will learn about other variants of the gradient descent process; in particular, we will learn about stochastic gradient descent, mini-batch gradient descent. We will also learn about various momentum based variants. But that is a topic for a later day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:08:59.989125Z",
     "start_time": "2021-10-14T20:08:59.983961Z"
    }
   },
   "outputs": [],
   "source": [
    "α = pow(10, -4)\n",
    "β_0, β_1 =  4, 4 #np.random.normal(0,5,2) # initialize the parameters to some random values.\n",
    "epochs = 200\n",
    "X, Y = data['eruptions'].values, data['waiting'].values\n",
    "α, β_0, β_1, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:00.492565Z",
     "start_time": "2021-10-14T20:09:00.036838Z"
    }
   },
   "outputs": [],
   "source": [
    "intermediates = pd.DataFrame(columns=['epoch', 'β0', 'β1', 'loss'])\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    \n",
    "    # compute the gradients\n",
    "    dβ_0    = -sum([yi - β_0 - β_1*xi for xi, yi in zip (X, Y)]) \n",
    "    dβ_1    = -sum([xi*(yi - β_0 - β_1*xi) for xi, yi in zip (X, Y)])\n",
    "    \n",
    "    # gradient descent step\n",
    "    β_0     = (β_0 - α*dβ_0)\n",
    "    β_1     = (β_1 - α*dβ_1)\n",
    "    \n",
    "    # update the loss function\n",
    "    loss    = sum([ (yi -β_0 -β_1*xi)**2 for xi, yi in zip (X, Y)])\n",
    "    \n",
    "    # store the values for later visualization\n",
    "    intermediates.loc[intermediates.shape[0]] = [epoch, β_0, β_1, loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us preview some of the rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:00.546327Z",
     "start_time": "2021-10-14T20:09:00.536863Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 3)     \n",
    "intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter optimization with epochs\n",
    "\n",
    "Let us now see how the learning of the parameters $\\beta_0$ and $\\beta_1$ happens with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:00.822406Z",
     "start_time": "2021-10-14T20:09:00.589645Z"
    },
    "cell_style": "split",
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(intermediates.epoch, intermediates.β0, linewidth=2, alpha=1, color='maroon');\n",
    "plt.xlabel(r'$Epoch\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_0\\longrightarrow$');\n",
    "plt.title(r\"\"\"$\\beta_0\\ vs\\ epochs$\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:01.644692Z",
     "start_time": "2021-10-14T20:09:01.418618Z"
    },
    "cell_style": "split",
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(intermediates.epoch, intermediates.β1, linewidth=2, alpha=1, color='maroon');\n",
    "plt.xlabel(r'$Epoch\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_1\\longrightarrow$');\n",
    "plt.title(r\"\"\"$\\beta_1\\ vs\\ epochs$\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let us see how the loss (sum squared errors) decreased with the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:03.344341Z",
     "start_time": "2021-10-14T20:09:03.044690Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(intermediates.epoch, intermediates.loss, linewidth=2, alpha=1, color='maroon');\n",
    "plt.xlabel(r'$Epoch\\longrightarrow$');\n",
    "plt.ylabel(r'$\\mathcal L\\longrightarrow$');\n",
    "plt.title(r\"\"\"$\\mathcal L\\ vs\\ epochs$\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T16:24:02.087543Z",
     "start_time": "2021-10-12T16:24:02.084409Z"
    },
    "cell_style": "center"
   },
   "source": [
    "## Loss contour plots in the parameter space\n",
    "\n",
    "Let us start by creating a grid mesh of points in the hypothesis space, each point corresponding to a particular hypothesis's parameter value $\\boldsymbol\\beta = \\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\end{pmatrix}$. Next, let us compute the value of the loss function, $\\mathscr{L}$, as the sum-squared errors at each of these points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:06.640670Z",
     "start_time": "2021-10-14T20:09:04.949450Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(β0:float, β1:float) -> float:\n",
    "     return sum ([(yi - β0 -β1*xi)**2 for xi,yi in zip(X, Y)])\n",
    "\n",
    "# Create the mesh grid of values.\n",
    "b0     = np.linspace(-4, 4, 1000)\n",
    "b1     = np.linspace(-4,4,1000)\n",
    "B0, B1 = np.meshgrid(b0, b1)\n",
    "rss    = loss(B0, B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the contour surfaces (curves of equal loss or errors). Because we had standardized the data, the contour plot is elliptic will low eccentricity, i.e. almost circular; however, because we have drawn the plot in a 2:1 ratio, it looks more elliptical than it is. Change the plot size below to a square (20,20) for the figure size, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:07.276382Z",
     "start_time": "2021-10-14T20:09:06.684154Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "levels=[0, 50, 100, 200, 400, 800, 1600, 3200, 6400, 10_000]\n",
    "plt.figure(figsize=(20,10))\n",
    "contour_plot = plt.contour(B0, B1, rss, levels, colors='black', linestyles='dashed', linewidths=1, )\n",
    "plt.clabel(contour_plot, inline=1, fontsize=10)\n",
    "contour_plot = plt.contourf(B0, B1, rss,alpha=0.7)\n",
    "plt.xlabel(r'$\\beta_0\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_1\\longrightarrow$');\n",
    "plt.title(r'\\textbf{Loss function contour plot}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss surface and the contour plots in the parameter plane ($\\boldsymbol \\beta$-hyperplane)\n",
    "\n",
    "see the relationship between the loss surface, and its projection on the $\\boldsymbol \\beta$-hyperplane. \n",
    "\n",
    "**Note: in order to show the loss surface above the contour plot, we have artificially added 2000 to the residual loss, in order to lift the loss surface. To see the correct loss surface, reset the `loss_lift` to zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T20:09:08.825488Z",
     "start_time": "2021-10-14T20:09:07.455292Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(20,30))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "loss_lift = 2000 # artificially lifing the loss surface to better show the contour plot below it.\n",
    "\n",
    "#Plot the loss surface\n",
    "ax.plot_surface(B0, B1, loss_lift + (rss),\n",
    "                cmap='viridis', alpha=0.9);\n",
    "\n",
    "levels=[0, 100, 200, 400, 800, 1600, 3200, 6400, 10_000]\n",
    "\n",
    "# Plot the contours\n",
    "ax.contourf(B0, B1, rss, levels, offset = 0, alpha=0.5)\n",
    "plt.xlabel(r'$\\beta_0\\longrightarrow$');\n",
    "plt.ylabel(r'$\\beta_1\\longrightarrow$');\n",
    "ax.set_zlabel(r'$Loss\\longrightarrow$');\n",
    "\n",
    "# Plot the learning journey\n",
    "ax.plot(intermediates.β0, intermediates.β1, intermediates.loss + loss_lift,  color='red',zdir='z', linewidth=6)\n",
    "ax.scatter(intermediates.β0, intermediates.β1, s=20,  color='salmon',zdir='z')\n",
    "plt.title(r'\\textbf {The loss surface and its contour plot}');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## GRADIENT DESCENT ANIMATION\n",
    "\n",
    "create an animation to show the progression of learning with gradient descent. In the video below, the process of gradient descent is shown as the gradual optimization takes place.\n",
    "\n",
    "*Caution: This code below to generate the animation can be computationally very intensive and take quite some time to finish. The value of the loop has deliberately been set to zero, so that you don't accidentally run it.*\n",
    "\n",
    "If you want to run it, uncomment the line of code below that sets the variable `iterations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T23:09:45.561189Z",
     "start_time": "2021-10-14T20:16:44.401694Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "fig = plt.figure(figsize=(20,30))\n",
    "camera = Camera(fig)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "loss_lift = 2000 \n",
    "\n",
    "iterations = 0                          # for safety, that this code does not accidentally get run.\n",
    "#iterations = intermediates.shape[0]+1  # uncomment this if you want to generate a new animation video.\n",
    "# Plot the learning journey\n",
    "for i in range(iterations):\n",
    "    ax.plot_surface(B0, B1, loss_lift + (rss),\n",
    "                cmap='viridis', alpha=0.9);\n",
    "\n",
    "    levels=[0, 100, 200, 400, 800, 1600, 3200, 6400, 10_000]\n",
    "\n",
    "    ax.contourf(B0, B1, rss, levels, offset = 0, alpha=0.5)\n",
    "    plt.xlabel(r'$\\beta_0\\longrightarrow$');\n",
    "    plt.ylabel(r'$\\beta_1\\longrightarrow$');\n",
    "    ax.set_zlabel(r'$Loss\\longrightarrow$');\n",
    "    subset = intermediates.head(i+1)\n",
    "    ax.plot(subset.β0, subset.β1, subset.loss + loss_lift,  color='red',zdir='z', linewidth=6)\n",
    "    ax.plot(subset.β0, subset.β1, linewidth=6,  color='salmon',zdir='z')\n",
    "    plt.title(r'\\textbf {The loss surface and its contour plot}');\n",
    "    plt.tight_layout()\n",
    "    camera.snap()\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('animation-gradient-descent.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
